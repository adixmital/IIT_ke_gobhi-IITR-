# -*- coding: utf-8 -*-
"""heart-rate-detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Av3FG9TZSMl_Nxf_HE9UTxzO5z6avQU0
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

from google.colab import drive
drive.mount('/content/drive')

"""# Loading Of Datasets

We have 3 files - train, test, sample. I am going to extract the HR column of the sample file which will be used as y_true for out test data.
"""

train=pd.read_csv('/content/train_data.csv')

test=pd.read_csv("/content/sample_test_data.csv")

test['condition'].value_counts()

sample=pd.read_csv('/content/sample_output_generated.csv')

y_true=sample['HR']

"""# Explotary Data Analysis

***Checking of Datatype and Missing values***
"""

train.info()

"""There are 2 object type columns and the target column is continous , therefore it is a linear regression problem. Also there is not missing values in the dataset.
we will drop the uuid column , there is no use of it.
"""

for columns in train.columns:
    print(train[columns].value_counts())

"""From the above value_counts you can understand that datasetId column is a constant column wich is equal to 2. So we will drop it."""

df=train.drop(['datasetId','uuid'],axis=1)
y=train['HR']



test1=test.drop(['datasetId','uuid'],axis=1)

"""I am also droping the uuid column which is just a ID column.

We have also a object column called condition , I am going to convert that column to numerical column by using a %allocation method. I am going to use the formula like

= (Count of that value)/(total values in that column)
"""

train['condition'].value_counts()
df['condition']=train['condition'].map({'no stress':0.5418,'interruption':0.2774,'time pressure':0.1808}) # check this

total_counts=test1['condition'].count()
count=test1['condition'].value_counts()
test1['condition']=test1['condition'].map(lambda x:count[x]/total_counts)

def condition_mapping_function(x):
    return count[x] / total_counts

df.shape

"""We can see that the target column here is continous, so this is a regression problem , there are many ML algorithms to solve it , like Linear Regression, DecisionTree Regression, RandomForest Regression , Support Vector Regression. Lets analyze each of them.

First we are removing highly correlated data columns
"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(y)

"""The most of the values of the heart-rates are gathered within the 55-85."""

corr=df.corr()

plt.figure(figsize=(16, 12))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
plt.show()

columns = np.full((corr.shape[0],), True, dtype=bool)
for i in range(corr.shape[0]):
    for j in range(i+1, corr.shape[0]):
        if corr.iloc[i,j] >= 0.8: # hyperparameter-1
            if columns[j]:
                columns[j] = False

selected_columns = df.columns[columns]
selected_columns.shape

df_new=df[selected_columns]

df_new.columns

sns.pairplot( df_new,x_vars=df_new.columns, y_vars='HR', size=7, aspect=0.7)

"""In linear regression the features will be linearly related to the target column , but in this case most of the features are not linear , so it linear regression will not able to capture all complexiciteis of the data. So we will first apply the non-linear models."""

df_new1=df_new.drop(['HR'],axis=1)

ref_features=df_new1.columns
target='HR'
variables=[ref_features,target]

sns.set(style="whitegrid")
plt.figure(figsize=(8, 8))
sns.boxplot(data=y, orient='v')
plt.xticks(rotation=90)
plt.title('Boxplot of Each Column')
plt.show()

"""You can see there are outliers in the target column but, we dont remove them , as humans can have heart rate greater than usual( >100) , so we dont remove them. I can say that very much people whose heart rate is determined are patients. Useful Infomation."""

test1=test1[df_new1.columns]

test1.columns

"""# Importing Necessary Libraries"""

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR

"""# StandardScaler"""

scaler=StandardScaler()
X1=scaler.fit_transform(df_new1)
test1=scaler.transform(test1)

scaler_params=[scaler.mean_,scaler.var_]

"""# Train Test Split"""

X_train,X_test,y_train,y_test=train_test_split(X1,y,test_size=0.2,random_state=42)

"""# Decision Tree"""

from sklearn.tree import DecisionTreeRegressor

dec_tree = DecisionTreeRegressor(random_state=0)
dec_tree.fit(X_train,y_train)
dec_tree_y_pred = dec_tree.predict(X_test)
print("Accuracy: {}".format(dec_tree.score(X_test,y_test)))
print("R squared: {}".format(r2_score(y_true=y_test,y_pred=dec_tree_y_pred)))

dec_tree.score(test1,y_true)

"""# Random Forest"""

from sklearn.ensemble import RandomForestRegressor

rf_tree = RandomForestRegressor(random_state=0)
rf_tree.fit(X_train,y_train)
rf_tree_y_pred = rf_tree.predict(X_test)
print("Accuracy: {}".format(rf_tree.score(X_test,y_test)))
print("R squared: {}".format(r2_score(y_true=y_test,y_pred=rf_tree_y_pred)))

rf_tree.score(test1,y_true)

"""# Support Vector Machine"""

from sklearn.svm import SVR

svr = SVR()
svr.fit(X_train,y_train)
svr_y_pred = svr.predict(X_test)
print("Accuracy: {}".format(svr.score(X_test,y_test)))
print("R squared: {}".format(r2_score(y_true=y_test,y_pred=svr_y_pred)))

svr.score(test1,y_true)

"""# GradientBoosting"""

from sklearn.ensemble import GradientBoostingRegressor
gbr=GradientBoostingRegressor()
gbr.fit(X_train,y_train)
gbr_y_pred = gbr.predict(X_test)
print("Accuracy: {}".format(gbr.score(X_test,y_test)))

gbr.score(test1,y_true)

"""# XGBoost Regressor"""

from xgboost import XGBRegressor
xgbr=XGBRegressor()
xgbr.fit(X_train,y_train)
xgbr_y_pred = xgbr.predict(X_test)
print("Accuracy: {}".format(xgbr.score(X_test,y_test)))

xgbr_pred=xgbr.predict(test1)

print("Accuracy: {}".format(xgbr.score(test1,y_true)))

"""# Linear Regression"""

lr=LinearRegression()
lr.fit(X_train,y_train)
lr_pred=lr.predict(X_test)
print("R squared: {}".format(r2_score(y_true=y_test,y_pred=lr_pred)))

lr_pred1=lr.predict(test1)
print("R squared: {}".format(r2_score(y_true=y_true,y_pred=lr_pred1)))

"""After seeing the performance of the each regressor , I am decided to use the ensemble of the model.The reason which would be like some base models, like Random Forest, Support Vector ,XGBoost, and Decision Tree, are effective at capturing nonlinear relationships as well as linear relationships and they all are also non-sensitive to outliers, while others, like Linear Regression, may perform better in capturing linear relationships but sensitive to outliers.As We all know ,in this case outliers are important. The ensemble can adapt to different types of relationships in the data as well as in the outliers.

# Voting Regressor
"""

from sklearn.ensemble import VotingRegressor
voting_reg = VotingRegressor(estimators=[('XGBoost', xgbr),  ('svr', svr), ('dct', dec_tree), ('rf', rf_tree)],
                             n_jobs=5)

# Fit the Voting Regressor on the training data
voting_reg.fit(X_train, y_train)

voting_reg.score(X_test,y_test)
voting_reg.score(test1,y_true)

testing=pd.DataFrame({'predictions':voting_reg.predict(X_test),'actual':y_test})

"""**Comparison of the predictions of the different models on the test data**"""

svr_pred=svr.predict(test1)
dec_tree_pred=dec_tree.predict(test1)
rf_tree_pred=rf_tree.predict(test1)
voting_reg_pred=voting_reg.predict(test1)
plt.figure()
plt.plot(lr_pred1,'y^',label="Linear Regression")
plt.plot(dec_tree_pred, "dc", label="DecisionTreeRegressor")
plt.plot(rf_tree_pred, "b^", label="RandomForestRegressor")
plt.plot(svr_pred, "g^", label="Support Vector Regressor")
plt.plot(xgbr_pred, "bo", label="XGBoostRegression")

plt.plot(voting_reg_pred, "r*", ms=10, label="VotingRegressor")
plt.plot( y_true,"y*--", label = 'Test Data Points')

plt.tick_params(axis="x", which="both", bottom=False, top=False, labelbottom=False)
plt.ylabel("predicted")
plt.xlabel("testing samples")
plt.legend(loc="best")
plt.title("Regressor predictions and their average")

plt.show()

results=pd.DataFrame({'predictions':voting_reg_pred,'actuals':y_true})
results

"""Though the model is good , As we have some outliers , it can be possible that we have some chances that our model will not work on the outliers, lets see the predictions vs actuals plot for X_test"""

sns.lineplot(testing,x=testing['predictions'],y=testing['actual'])
sns.lineplot(testing,x=testing['actual'],y=testing['actual'])

"""You can see here that the cache here that , when the heart rate in the actually greater than 100 our model fails to predict , basically our model is not performing well with outliers."""

len(train[train['HR']>=100])

"""# Final Model

I am going to store the final model as voting regresssor , the reference columns,the conditioning function used for transforming the *Condition* column to numerical column  the scaler parameters and the label as finalized_model.sav
"""

# import joblib

# filename = 'finalized_model.pkl'
# joblib.dump((voting_reg,variables,condition_mapping_function,scaler_params), filename)

import pickle

# with open ('voting_reg.pkl','wb') as file:
#     pickle.dump(voting_reg,file)